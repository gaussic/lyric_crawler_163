{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename, mode='r'):\n",
    "    return open(filename, mode=mode, encoding='utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    words = open_file(vocab_path).read().strip().split('\\n')\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def build_vocab(data_path, vocab_path, vocab_size):\n",
    "    tokens = ['<sos>', '<eos>', '<unk>']  # 词汇表中的几个重要标记\n",
    "    all_words = open_file(data_path).read().strip().replace('\\n', ' ').split()\n",
    "    count_pairs = Counter(all_words).most_common(vocab_size)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    tokens += words\n",
    "    open_file(vocab_path, 'w').write('\\n'.join(tokens) + '\\n')\n",
    "\n",
    "\n",
    "def text_to_id(text, w2id, unk_token):\n",
    "    return [w2id[x] if x in w2id else unk_token for x in text.split()]\n",
    "\n",
    "\n",
    "def id_to_text(ids, words):\n",
    "    return ' '.join([words[x] for x in ids])\n",
    "\n",
    "def variables_from_ids(ids):\n",
    "    ids_var = Variable(torch.LongTensor(ids))\n",
    "    if use_cuda:\n",
    "        ids_var = ids_var.cuda()\n",
    "    return ids_var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_var = Variable(torch.LongTensor(pair[0]))\n",
    "    target_var = Variable(torch.LongTensor(pair[1]))\n",
    "    if use_cuda:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "    return (input_var, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, data_path, vocab_path, vocab_size=10000):\n",
    "        assert os.path.exists(data_path)\n",
    "\n",
    "        if not os.path.exists(vocab_path):\n",
    "            build_vocab(data_path, vocab_path, vocab_size - 3)\n",
    "\n",
    "        self.words, self.word_to_id = read_vocab(vocab_path)\n",
    "\n",
    "        self.tokenize(data_path)\n",
    "\n",
    "    def tokenize(self, data_path):\n",
    "        unk_token = self.word_to_id['<unk>']\n",
    "        lines = []\n",
    "        data = []\n",
    "        for line in open_file(data_path):\n",
    "            if len(line.strip()) == 0:\n",
    "                data.extend(list(zip(lines[:-1], lines[1:])))\n",
    "                lines = []\n",
    "            line_ids = text_to_id(line + ' <eos>', self.word_to_id, unk_token)\n",
    "            if line_ids.count(unk_token) < len(line_ids) * 0.2:\n",
    "                lines.append(line_ids)\n",
    "\n",
    "        # 打乱，分离数据集\n",
    "        random.shuffle(data)\n",
    "        data_len = len(data)\n",
    "        #self.data_train = data[:int(0.7 * data_len)]\n",
    "        #self.data_val = data[int(0.7 * data_len):int(0.8 * data_len)]\n",
    "        #self.data_test = data[int(0.8 * data_len):]\n",
    "        \n",
    "        self.data_train = [variables_from_pair(pair) for pair in data[:7000]]\n",
    "        self.data_val = [variables_from_pair(pair) for pair in data[7000:8000]]\n",
    "        self.data_test = [variables_from_pair(pair) for pair in data[8000:10000]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Vocab size: {}\\nTrain len: {}\\nValidation len: {}\\nTest len: {}\".format(len(self.words),\n",
    "                                                                                        len(self.data_train),\n",
    "                                                                                        len(self.data_val),\n",
    "                                                                                        len(self.data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10000\n",
      "Train len: 7000\n",
      "Validation len: 1000\n",
      "Test len: 2000\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus('lyric_full.txt', 'lyric_vocab.txt')\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "承受 不了 你 的 善变 我 <unk> <eos>\n",
      "不管 对 不 对 是 对 爱 不想 有所 违背 <eos>\n"
     ]
    }
   ],
   "source": [
    "r_t = random.choice(corpus.data_train)\n",
    "print(id_to_text(r_t[0].data.cpu().numpy(), corpus.words))\n",
    "print(id_to_text(r_t[1].data.cpu().numpy(), corpus.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input_s, hidden=None):\n",
    "        embedded = self.embedding(input_s).view(len(input_s), 1, -1)\n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_s, last_hidden):\n",
    "        # Note: we run this one step at a time\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input_s)\n",
    "        embedded = self.embedding_dropout(embedded).view(1, 1, -1)\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.rnn(embedded, last_hidden)\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(rnn_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "\n",
    "vocab_size = len(corpus.words)\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "encoder_test = EncoderRNN(embedding, hidden_size, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(10000, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (rnn): LSTM(500, 500, num_layers=2, dropout=0.1)\n",
       "  (out): Linear(in_features=500, out_features=10000)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_test = DecoderRNN(embedding, hidden_size, vocab_size, n_layers)\n",
    "decoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    encoder_test.cuda()\n",
    "    decoder_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7]) torch.Size([8])\n",
      "torch.Size([7, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n",
      "torch.Size([1, 10000]) torch.Size([2, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "input_var, target_var = random.choice(corpus.data_train)\n",
    "print(input_var.size(), target_var.size())\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_var)\n",
    "print(encoder_outputs.size())\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "for i in range(len(target_var)):\n",
    "    decoder_output, decoder_hidden = decoder_test(target_var[i], decoder_hidden)\n",
    "    print(decoder_output.size(), decoder_hidden[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    target_length = target_variable.size(0)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable)\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[corpus.word_to_id['<sos>']]]))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        decoder_input = target_variable[di] \n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "clip = 5\n",
    "decoder_learning_ratio = 5.0\n",
    "encoder_optimizer = optim.Adam(encoder_test.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder_test.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.208130836486816"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = train(input_var, target_var, encoder_test, decoder_test, encoder_optimizer, decoder_optimizer, criterion)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(max_length=20):\n",
    "    input_var, target_var = random.choice(corpus.data_test)\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder_test.eval()\n",
    "    decoder_test.eval()\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder_test(input_var)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([corpus.word_to_id['<sos>']]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder_test.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder_test(\n",
    "            decoder_input, decoder_hidden\n",
    "        )\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == corpus.word_to_id['<eos>']:\n",
    "            decoded_words.append('<eos>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(corpus.words[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if use_cuda: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder_test.train()\n",
    "    decoder_test.train()\n",
    "    \n",
    "    print(id_to_text(pair[0], corpus.words))\n",
    "    print(id_to_text(pair[1], corpus.words))\n",
    "    print(' '.join(decoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_one(input_variable, target_variable, encoder, decoder, criterion):\n",
    "    loss = 0\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    target_length = target_variable.size(0)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable)\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[corpus.word_to_id['<sos>']]]))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        decoder_input = target_variable[di] \n",
    "        \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "def evaluate_full(eval_data):\n",
    "    total_loss = 0.0\n",
    "    for pair in eval_data:\n",
    "        input_var, target_var = pair\n",
    "        total_loss += evaluate_one(input_var, target_var, encoder_test, decoder_test, criterion)\n",
    "    total_loss /= len(eval_data)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print_per_epoch = 200\n",
    "# epochs = 50000\n",
    "# total_loss = 0\n",
    "# for i in range(1, epochs):\n",
    "#     input_var, target_var = variables_from_pair(random.choice(corpus.data_train))\n",
    "#     total_loss += train(input_var, target_var, encoder_test, decoder_test, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#     if i % print_per_epoch == 0:\n",
    "#         print(i, total_loss / print_per_epoch)\n",
    "#         total_loss = 0\n",
    "        \n",
    "#     if i % 2000 == 0:\n",
    "#         print(evaluate_full(corpus.data_val))\n",
    "#         print(evaluate_full(corpus.data_test))\n",
    "#         evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0m 0s'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "as_minutes(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 6.949021146893936 0m 28s\n",
      "1000 6.567773572145218 0m 56s\n"
     ]
    }
   ],
   "source": [
    "print_per_epoch = 500\n",
    "epochs = 50000\n",
    "total_loss = 0\n",
    "print_loss = 0\n",
    "for i in range(len(corpus.data_train)):\n",
    "    input_var, target_var = corpus.data_train[i]\n",
    "    cur_loss = train(input_var, target_var, encoder_test, decoder_test, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    total_loss += cur_loss\n",
    "    print_loss += cur_loss\n",
    "    if i % print_per_epoch == 0 and i > 0:\n",
    "        print(i, print_loss / print_per_epoch, as_minutes(time.time() - start_time))\n",
    "        print_loss = 0\n",
    "        \n",
    "    if i % 2000 == 0 and i > 0:\n",
    "        print(evaluate_full(corpus.data_val))\n",
    "        print(evaluate_full(corpus.data_test))\n",
    "        evaluate()\n",
    "        print(as_minutes(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_full(corpus.data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
