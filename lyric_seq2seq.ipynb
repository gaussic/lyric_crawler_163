{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename, mode='r'):\n",
    "    return open(filename, mode=mode, encoding='utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    words = open_file(vocab_path).read().strip().split('\\n')\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def build_vocab(data_path, vocab_path, vocab_size):\n",
    "    tokens = ['<sos>', '<eos>', '<unk>']  # 词汇表中的几个重要标记\n",
    "    all_words = open_file(data_path).read().strip().replace('\\n', ' ').split()\n",
    "    count_pairs = Counter(all_words).most_common(vocab_size)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    tokens += words\n",
    "    open_file(vocab_path, 'w').write('\\n'.join(tokens) + '\\n')\n",
    "\n",
    "\n",
    "def text_to_id(text, w2id, unk_token):\n",
    "    return [w2id[x] if x in w2id else unk_token for x in text.split()]\n",
    "\n",
    "\n",
    "def id_to_text(ids, words):\n",
    "    return ' '.join([words[x] for x in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, data_path, vocab_path, vocab_size=10000):\n",
    "        assert os.path.exists(data_path)\n",
    "\n",
    "        if not os.path.exists(vocab_path):\n",
    "            build_vocab(data_path, vocab_path, vocab_size - 3)\n",
    "\n",
    "        self.words, self.word_to_id = read_vocab(vocab_path)\n",
    "\n",
    "        self.tokenize(data_path)\n",
    "\n",
    "    def tokenize(self, data_path):\n",
    "        eos_token = self.word_to_id['<eos>']\n",
    "        unk_token = self.word_to_id['<unk>']\n",
    "        lines = []\n",
    "        data = []\n",
    "        for line in open_file(data_path):\n",
    "            if len(line.strip()) == 0:\n",
    "                data.extend(list(zip(lines[:-1], lines[1:])))\n",
    "                lines = []\n",
    "            line_ids = text_to_id(line + ' <eos>', self.word_to_id, unk_token)\n",
    "            if line_ids.count(unk_token) < len(line_ids) * 0.2:\n",
    "                lines.append(line_ids)\n",
    "\n",
    "        # 打乱，分离数据集\n",
    "        random.shuffle(data)\n",
    "        data_len = len(data)\n",
    "        self.data_train = data[:int(0.7 * data_len)]\n",
    "        self.data_val = data[int(0.7 * data_len):int(0.8 * data_len)]\n",
    "        self.data_test = data[int(0.8 * data_len):]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Vocab size: {}\\nTrain len: {}\\nValidation len: {}\\nTest len: {}\".format(len(self.words),\n",
    "                                                                                        len(self.data_train),\n",
    "                                                                                        len(self.data_val),\n",
    "                                                                                        len(self.data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('lyric_full.txt', 'lyric_vocab.txt')\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_t = random.choice(corpus.data_train)\n",
    "print(id_to_text(r_t[0], corpus.words))\n",
    "print(id_to_text(r_t[1], corpus.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input_s, hidden=None):\n",
    "        embedded = self.embedding(input_s).view(len(input_s), 1, -1)\n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "\n",
    "vocab_size = len(corpus.words)\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "encoder_test = EncoderRNN(embedding, hidden_size, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_s, last_hidden):\n",
    "        # Note: we run this one step at a time\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input_s)\n",
    "        embedded = self.embedding_dropout(embedded).view(1, 1, -1)\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.rnn(embedded, last_hidden)\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(rnn_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_test = DecoderRNN(embedding, hidden_size, vocab_size, n_layers)\n",
    "decoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variables_from_pair(pair):\n",
    "    input_var = Variable(torch.LongTensor(pair[0]))\n",
    "    target_var = Variable(torch.LongTensor(pair[1]))\n",
    "    if use_cuda:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "    return (input_var, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = random.choice(corpus.data_train)\n",
    "input_var, target_var = variables_from_pair(pair)\n",
    "print(input_var.size(), target_var.size())\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_var)\n",
    "print(encoder_outputs.size())\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "for i in range(len(target_var)):\n",
    "    decoder_output, decoder_hidden = decoder_test(target_var[i], decoder_hidden)\n",
    "    print(decoder_output.size(), decoder_hidden[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    target_length = target_variable.size(0)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable)\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[corpus.word_to_id['<sos>']]]))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        decoder_input = target_variable[di] \n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "clip = 5.0\n",
    "decoder_learning_ratio = 5.0\n",
    "encoder_optimizer = optim.Adam(encoder_test.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder_test.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(input_var, target_var, encoder_test, decoder_test, encoder_optimizer, decoder_optimizer, criterion)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_per_epoch = 10\n",
    "epochs = 50000\n",
    "total_loss = 0\n",
    "for i in range(1, epochs):\n",
    "    input_var, target_var = variables_from_pair(random.choice(corpus.data_train))\n",
    "    total_loss += train(input_var, target_var, encoder_test, decoder_test, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    if i % print_per_epoch == 0:\n",
    "        print(i, total_loss / print_per_epoch)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
